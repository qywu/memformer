{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from collections import OrderedDict\n",
    "from transformers import BartTokenizer, PretrainedConfig, BartForConditionalGeneration\n",
    "from memformers.models.membart import MemBartForConditionalGeneration, MemBartModel\n",
    "\n",
    "from memformers.models.membart.utils import get_model_config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = PretrainedConfig.from_dict(get_model_config(\"membart-base.yaml\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_weights(state_dict):\n",
    "    new_state_dict = OrderedDict()\n",
    "    for key, value in state_dict.items():\n",
    "        key = key.replace(\"recurrent_training_cell.cell.\", \"\")\n",
    "        new_state_dict[key] = value\n",
    "    return new_state_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = torch.load(\"./data/base/iter_156335_model_state.pth\", map_location=\"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at qywu/membart-base were not used when initializing MemBartModel: ['final_logits_bias', 'lm_head.weight']\n",
      "- This IS expected if you are initializing MemBartModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MemBartModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = MemBartModel.from_pretrained(\"qywu/membart-base\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MemBartForConditionalGeneration(model_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_IncompatibleKeys(missing_keys=['final_logits_bias', 'lm_head.weight'], unexpected_keys=[])\n"
     ]
    }
   ],
   "source": [
    "state_dict = process_weights(state_dict)\n",
    "print(model.load_state_dict(state_dict, strict=False))\n",
    "model.tie_weights()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-base\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = \"\"\"\n",
    "Barack Obama served as the 44th President of the United States.\n",
    "\"\"\"\n",
    "\n",
    "text2 = \"\"\"\n",
    "<mask> served as the 44th President of the United States.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory_states = model.construct_memory(1)\n",
    "\n",
    "# t = 0\n",
    "# memory generation step\n",
    "input_ids = torch.LongTensor([tokenizer.encode(text1, add_special_tokens=True)])\n",
    "encoder_outputs = model.model.encoder(input_ids=input_ids, memory_states=memory_states, attention_mask=None)\n",
    "\n",
    "memory_states = encoder_outputs.memory_states\n",
    "\n",
    "# input_ids = torch.LongTensor([tokenizer.encode(text2,\n",
    "#                                                add_special_tokens=True)])\n",
    "\n",
    "# encoder_outputs = model.model.encoder(input_ids=input_ids,\n",
    "#                     memory_states=memory_states,\n",
    "#                     attention_mask=None)\n",
    "\n",
    "# memory_states = encoder_outputs.memory_states\n",
    "\n",
    "# # input_ids = torch.LongTensor([tokenizer.encode(\"g g h h i i\",\n",
    "# #                                                add_special_tokens=True)])\n",
    "\n",
    "# # encoder_outputs = model.model.encoder(input_ids=input_ids,\n",
    "# #                     memory_states=memory_states,\n",
    "# #                     attention_mask=None)\n",
    "\n",
    "# # memory_states = encoder_outputs.memory_states\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s><s> Barack Obama served as the 44th President of the United States.\\n</s>'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# t = 1\n",
    "# without memory states\n",
    "input_ids2 = torch.LongTensor([tokenizer.encode(text2, add_special_tokens=True)])\n",
    "\n",
    "encoder_outputs2 = model.model.encoder(input_ids=input_ids2, memory_states=memory_states, attention_mask=None)\n",
    "\n",
    "outputs = model.generate(\n",
    "    encoder_outputs=encoder_outputs2,\n",
    "    decoder_start_token_id=tokenizer.bos_token_id,\n",
    "    max_length=64,\n",
    "    num_beams=4,\n",
    "    do_sample=False,\n",
    "    return_dict_in_generate=True,\n",
    ")\n",
    "\n",
    "tokenizer.decode(outputs.sequences[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "bart_model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-base\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "Cindy has a book called \"A\"\n",
    "\n",
    "Mary has a book called \"B\"\n",
    "\n",
    "<extra_id_0> has a book called \"B\"\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s><s>Cindy has a book called \"A\" and \"B\" in her book.Mary has a phone number.<extra_id_</s>'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = torch.LongTensor([tokenizer.encode(text, add_special_tokens=True)])\n",
    "\n",
    "outputs = bart_model.generate(\n",
    "    input_ids,\n",
    "    decoder_start_token_id=tokenizer.bos_token_id,\n",
    "    max_length=32,\n",
    "    num_beams=4,\n",
    "    do_sample=False,\n",
    "    return_dict_in_generate=True,\n",
    ")\n",
    "\n",
    "tokenizer.decode(outputs.sequences[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qywu/anaconda3/lib/python3.9/site-packages/transformers/models/t5/tokenization_t5.py:163: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "t5_model = T5ForConditionalGeneration.from_pretrained(\"t5-base\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<pad><extra_id_0> Mary<extra_id_1> Cindy has a book called \"A\" Mary has a book called \"A\" Mary<extra_id_2> Mary<extra_id_3> Cindy<extra_id_4>Cindy has a'"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = torch.LongTensor([tokenizer.encode(text, add_special_tokens=True)])\n",
    "\n",
    "outputs = t5_model.generate(\n",
    "    input_ids,\n",
    "    decoder_start_token_id=tokenizer.bos_token_id,\n",
    "    max_length=32,\n",
    "    num_beams=4,\n",
    "    do_sample=False,\n",
    "    return_dict_in_generate=True,\n",
    ")\n",
    "\n",
    "tokenizer.decode(outputs.sequences[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token is valid.\n",
      "Your token has been saved in your configured git credential helpers (cache).\n",
      "Your token has been saved to /home/qywu/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "# upload a model to huggingface.co/models\n",
    "\n",
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pytorch_model.bin: 100%|██████████| 731M/731M [01:02<00:00, 11.8MB/s] \n",
      "Upload 1 LFS files: 100%|██████████| 1/1 [01:02<00:00, 62.17s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/qywu/membart-base/commit/844d9fec91e004f9fad0708ff3a9b70074f91535', commit_message='Upload MemBartForConditionalGeneration', commit_description='', oid='844d9fec91e004f9fad0708ff3a9b70074f91535', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.push_to_hub(\"qywu/membart-base\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/qywu/membart-base/commit/ce6654163ae033f54791fad168fc60605cafb10e', commit_message='Upload tokenizer', commit_description='', oid='ce6654163ae033f54791fad168fc60605cafb10e', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.push_to_hub(\"qywu/membart-base\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)olve/main/vocab.json: 100%|██████████| 899k/899k [00:00<00:00, 10.6MB/s]\n",
      "Downloading (…)olve/main/merges.txt: 100%|██████████| 456k/456k [00:00<00:00, 8.43MB/s]\n",
      "Downloading (…)okenizer_config.json: 100%|██████████| 26.0/26.0 [00:00<00:00, 12.8kB/s]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bos_token': '<s>',\n",
       " 'eos_token': '</s>',\n",
       " 'unk_token': '<unk>',\n",
       " 'sep_token': '</s>',\n",
       " 'pad_token': '<pad>',\n",
       " 'cls_token': '<s>',\n",
       " 'mask_token': '<mask>'}"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.special_tokens_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "29971f1763eb045a50ba33f0b6018d366307b50e59f009ec46fbd4e0e484bada"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
